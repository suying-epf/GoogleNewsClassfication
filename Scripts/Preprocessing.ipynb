{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'indexer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sn/t953ndtj629fq98hd__rjp7r0000gn/T/ipykernel_85783/3823609723.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFunctionTransformer\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspellchecker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpellChecker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stopwords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spellchecker/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# -*- coding: utf-8 -*-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m  \u001b[0mspellchecker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpellchecker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgetInstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spellchecker/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDictionaryIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangdetect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_detect_lang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'indexer'"
     ]
    }
   ],
   "source": [
    "import re  # noqa: F401\n",
    "import string  # noqa: F401\n",
    "\n",
    "import nltk  # noqa: F401\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords, wordnet  # noqa: F401\n",
    "from nltk.stem import WordNetLemmatizer  # noqa: F401\n",
    "from sklearn.pipeline import Pipeline  # noqa: F401\n",
    "from sklearn.preprocessing import FunctionTransformer  # noqa: F401\n",
    "from collections import Counter\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase\n",
    "def Lowercase(text: str) -> str:\n",
    "    return str(text).lower()\n",
    "\n",
    "# removal of Punctuation\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    \n",
    "    translation_table = str.maketrans('', '', PUNCT_TO_REMOVE)\n",
    "    return text.translate(translation_table)\n",
    "\n",
    "# Stopwords removal\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    \n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in STOPWORDS]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# lemmatization\n",
    "def lemmatize_words(text: str) -> str:\n",
    "\n",
    "    wordnet_map = {\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV,\n",
    "        'J': wordnet.ADJ\n",
    "    }\n",
    "    pos_tagged_text = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    lemmatized_pos_tagged_text = [lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text]\n",
    "    return ' '.join(lemmatized_pos_tagged_text)\n",
    "'''\n",
    "# emoticons conversion\n",
    "def convert_emoticons(text :str) -> str:\n",
    "\n",
    "    EMOTICONS = emoticons()\n",
    "    for emoticon, description in EMOTICONS.items():\n",
    "        cleaned_description = re.sub(\",\", \"\", description)\n",
    "        joined_description = \"_\".join(cleaned_description.split())\n",
    "        pattern = u'('+re.escape(emoticon)+')'\n",
    "        text = re.sub(pattern, joined_description, text)\n",
    "    return text\n",
    "\n",
    "# emoji conversion\n",
    "def convert_emojis(text :str) -> str:\n",
    "\n",
    "    EMO_UNICODE = emojis_unicode()\n",
    "    for emoji_code, emoji in EMO_UNICODE.items():\n",
    "        description = emoji_code.strip(\":\")  \n",
    "        no_commas = re.sub(\",\", \"\", description)\n",
    "        joined_description = \"_\".join(no_commas.split())\n",
    "        pattern = u'('+re.escape(emoji)+')'\n",
    "        text = re.sub(pattern, joined_description, text)\n",
    "    return text\n",
    "\n",
    "# urls removal\n",
    "def remove_urls(text :str) -> str:\n",
    "\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "# html tags removal\n",
    "def remove_html(text :str) -> str:\n",
    "    \n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)\n",
    "\n",
    "# chat words conversion\n",
    "def chat_words_conversion(text: str) -> str:\n",
    "    slang_words_list = slang_words()\n",
    "    chat_words_list = list(slang_words_list.keys())\n",
    "    new_text = []\n",
    "    \n",
    "    for word in text.split():\n",
    "        if word.upper() in chat_words_list:\n",
    "            new_text.append(slang_words_list[word.upper()])\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "\n",
    "    return ' '.join(new_text)\n",
    "'''\n",
    "#spelling correction\n",
    "def correct_spellings(text: str) -> str:\n",
    "    \n",
    "    spell = SpellChecker()\n",
    "    corrected_text = []\n",
    "    \n",
    "    misspelled = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled:\n",
    "            corrected_word = spell.correction(word)\n",
    "            corrected_text.append(corrected_word if corrected_word is not None else word)\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "\n",
    "\n",
    "    return ' '.join(corrected_text)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Transformer for each function\n",
    "lowercase_transformer = FunctionTransformer(np.vectorize(Lowercase))\n",
    "punctuation_transformer = FunctionTransformer(np.vectorize(remove_punctuation))\n",
    "stopwords_transformer = FunctionTransformer(np.vectorize(remove_stopwords))\n",
    "lemmatize_transformer = FunctionTransformer(np.vectorize(lemmatize_words))\n",
    "#emoticons_transformer = FunctionTransformer(np.vectorize(convert_emoticons))\n",
    "#emojis_transformer = FunctionTransformer(np.vectorize(convert_emojis))\n",
    "#urls_transformer = FunctionTransformer(np.vectorize(remove_urls))\n",
    "#html_transformer = FunctionTransformer(np.vectorize(remove_html))\n",
    "#chat_words_transformer = FunctionTransformer(np.vectorize(chat_words_conversion))\n",
    "#spellings_transformer = FunctionTransformer(np.vectorize(correct_spellings))\n",
    "\n",
    "# Combining transformers into a sklearn pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('lowercase', lowercase_transformer),\n",
    "    #('correct_spellings', spellings_transformer),\n",
    "    #('remove_html', html_transformer),\n",
    "    #('remove_urls', urls_transformer),\n",
    "    #('convert_emoticons', emoticons_transformer),\n",
    "    #('convert_emojis', emojis_transformer),\n",
    "    ('remove_punctuation', punctuation_transformer),\n",
    "    #('chat_words_conversion', chat_words_transformer),\n",
    "    ('remove_stopwords', stopwords_transformer),\n",
    "    ('lemmatize_words', lemmatize_transformer)\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(\"../Data/2023_9.csv\")\n",
    "    # df[\"cleaned_text\"] = df.text.apply(lambda x: pipeline(x))\n",
    "    df[\"cleaned_title\"] = pipeline.transform(df[\"Title\"].values) \n",
    "    df.to_csv(\"cleaned_Data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 18:29:29) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0057a9cdf74f52461b0e989bfb37ce9967b95ad68ac65a63196c7dd8320cc858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
